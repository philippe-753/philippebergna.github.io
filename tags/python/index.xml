<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python | Philippe Bergna - Academic CV</title>
    <link>https://philippebergna.github.io/tags/python/</link>
      <atom:link href="https://philippebergna.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Python</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 24 Oct 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://philippebergna.github.io/media/icon_hu7729264130191091259.png</url>
      <title>Python</title>
      <link>https://philippebergna.github.io/tags/python/</link>
    </image>
    
    <item>
      <title>Aversarial Attacks Introduction</title>
      <link>https://philippebergna.github.io/blogs/what-is-adversarial-attakcks/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://philippebergna.github.io/blogs/what-is-adversarial-attakcks/</guid>
      <description>&lt;h1 id=&#34;introduction-to-adversarial-attacks-in-computer-vision&#34;&gt;Introduction to Adversarial Attacks in Computer Vision&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Building Trust and Robustness in AI Models through Understanding Adversarial Vulnerabilities&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For the past 2.5 years, I have been engaged as a research scientist focusing on AI safety for computer vision systems at 
. In recent years, the surge in popularity of Large Language Models (LLMs) such as ChatGPT, Gemini, and Claude has shifted the attention of the AI safety community towards ensuring the reliability and security of these sophisticated language-based systems. However, I contend that research on adversarial attacks in computer vision remains a critical milestone for building trust and developing robust AI models.&lt;/p&gt;
&lt;p&gt;Adversarial attacks expose fundamental vulnerabilities in machine learning models, particularly in computer vision, by introducing subtle perturbations to input data that lead to incorrect model predictions. Understanding these attacks is essential not only for enhancing the security of AI systems but also for advancing the overall field of machine learning by highlighting areas where models may fail unexpectedly.&lt;/p&gt;
&lt;p&gt;This blog serves as an introductory exploration of adversarial attacks, delving into their definitions, underlying causes, various types, and significant research contributions. Whether you are an AI practitioner, researcher, or enthusiast, this discussion aims to provide a comprehensive foundation for understanding and addressing adversarial vulnerabilities in computer vision systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-are-adversarial-attacks&#34;&gt;What Are Adversarial Attacks?&lt;/h2&gt;
&lt;p&gt;Adversarial attacks are intentional manipulations of input data designed to deceive machine learning models into making incorrect predictions or classifications. These manipulations are often imperceptible to humans but can significantly alter the model&amp;rsquo;s output. The phenomenon was first widely recognized in the context of image classification tasks but extends to various domains within machine learning.&lt;/p&gt;
&lt;h3 id=&#34;formal-definition&#34;&gt;Formal Definition&lt;/h3&gt;
&lt;p&gt;Given a machine learning model \( f \) that maps input data \( x \) to output predictions \( f(x) \), an adversarial attack seeks to find a perturbed input \( x&#39; \) such that:&lt;/p&gt;
$$
x&#39; = x + \delta \quad \text{where} \quad \| \delta \| &lt; \epsilon \quad \text{and} \quad f(x&#39;) \neq f(x)
$$&lt;p&gt;Here, \( \delta \) represents the perturbation added to the original input \( x \), and \( \epsilon \) defines the maximum allowable magnitude of this perturbation, typically measured using norms such as \( \ell_\infty \) or \( \ell_2 \).&lt;/p&gt;
&lt;h3 id=&#34;real-world-implications&#34;&gt;Real-World Implications&lt;/h3&gt;
&lt;p&gt;Adversarial attacks pose significant risks in applications where AI systems are deployed in safety-critical environments. For instance, in autonomous driving, a minor alteration to a traffic sign could lead to misclassification, resulting in potentially hazardous driving decisions. Similarly, in security systems employing facial recognition, adversarial examples could bypass authentication mechanisms, compromising system integrity.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://your-image-link.com/adversarial_example.png&#34; alt=&#34;Adversarial Example&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
&lt;em&gt;Figure 1: An adversarial image where minor alterations deceive the AI into misclassification.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-do-adversarial-attacks-exist&#34;&gt;Why Do Adversarial Attacks Exist?&lt;/h2&gt;
&lt;p&gt;The existence of adversarial attacks raises profound questions about the nature of machine learning models and their decision-making processes. Despite extensive research, the precise reasons behind the susceptibility of models to such minimal perturbations remain partially understood, making this an active area of investigation.&lt;/p&gt;
&lt;h3 id=&#34;theoretical-perspectives&#34;&gt;Theoretical Perspectives&lt;/h3&gt;
&lt;p&gt;Several hypotheses have been proposed to explain why adversarial attacks are effective:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;High-Dimensional Spaces:&lt;/strong&gt; In high-dimensional input spaces, such as images, there exist numerous directions in which input data can be perturbed without significantly altering human perception. This abundance of directions provides adversaries with ample opportunities to find perturbations that mislead models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linear Behavior in Non-Linear Models:&lt;/strong&gt; Despite the non-linear nature of deep neural networks, these models exhibit locally linear behavior in high-dimensional spaces. This linearity can be exploited by adversaries to craft perturbations using gradient-based methods effectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Overfitting to Training Data:&lt;/strong&gt; Models that overfit to training data may capture noise and irrelevant patterns, making them more vulnerable to adversarial manipulations that exploit these specific learned patterns.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Complexity and Decision Boundaries:&lt;/strong&gt; The complexity of model architectures can lead to intricate decision boundaries. Adversarial perturbations can navigate these boundaries to cross into regions associated with different classes with minimal input changes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;empirical-evidence&#34;&gt;Empirical Evidence&lt;/h3&gt;
&lt;p&gt;Empirical studies demonstrate that even state-of-the-art models, such as convolutional neural networks (CNNs), are highly susceptible to adversarial attacks. The consistent success of various attack strategies across different models and datasets underscores the pervasive nature of this vulnerability.&lt;/p&gt;
&lt;h3 id=&#34;implications-for-ai-safety&#34;&gt;Implications for AI Safety&lt;/h3&gt;
&lt;p&gt;Understanding the underlying causes of adversarial vulnerabilities is crucial for developing effective defense mechanisms. Addressing these vulnerabilities is essential for ensuring the reliability and safety of AI systems, particularly as they become increasingly integrated into critical applications.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;types-of-adversarial-attacks&#34;&gt;Types of Adversarial Attacks&lt;/h2&gt;
&lt;p&gt;Adversarial attacks can be categorized based on several criteria, including the attacker&amp;rsquo;s knowledge of the target model, the attack&amp;rsquo;s objective, and the nature of the perturbations. Below, we outline the primary classifications and notable attack methods within each category.&lt;/p&gt;
&lt;h3 id=&#34;1-based-on-knowledge-of-the-target-model&#34;&gt;1. Based on Knowledge of the Target Model&lt;/h3&gt;
&lt;h4 id=&#34;a-white-box-attacks&#34;&gt;a. White-Box Attacks&lt;/h4&gt;
&lt;p&gt;In white-box attacks, the adversary has complete access to the target model&amp;rsquo;s architecture, parameters, and gradients. This comprehensive knowledge allows for precise crafting of adversarial examples by leveraging detailed insights into the model&amp;rsquo;s behavior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast Gradient Sign Method (FGSM)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Projected Gradient Descent (PGD)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Carlini &amp;amp; Wagner (C&amp;amp;W) Attack&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;b-black-box-attacks&#34;&gt;b. Black-Box Attacks&lt;/h4&gt;
&lt;p&gt;Black-box attacks operate under the assumption that the adversary has no internal knowledge of the model. Instead, the attacker can only interact with the model through input-output queries. These attacks often rely on surrogate models or query-based strategies to approximate the target model&amp;rsquo;s behavior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Zeroth-Order Optimization (ZOO)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transfer-Based Attacks&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Boundary Attack&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-based-on-the-attack-objective&#34;&gt;2. Based on the Attack Objective&lt;/h3&gt;
&lt;h4 id=&#34;a-targeted-attacks&#34;&gt;a. Targeted Attacks&lt;/h4&gt;
&lt;p&gt;Targeted attacks aim to misclassify an input into a specific, predetermined class. The adversary&amp;rsquo;s goal is not merely to cause any misclassification but to ensure the model predicts a particular incorrect label.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Altering an image of a &amp;ldquo;dog&amp;rdquo; so that the model classifies it as a &amp;ldquo;cat.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;b-untargeted-attacks&#34;&gt;b. Untargeted Attacks&lt;/h4&gt;
&lt;p&gt;Untargeted attacks seek to cause any misclassification without a specific target class in mind. The objective is to disrupt the model&amp;rsquo;s correct predictions, irrespective of the incorrect label assigned.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Modifying an image of a &amp;ldquo;car&amp;rdquo; so that the model fails to recognize it as a &amp;ldquo;car,&amp;rdquo; potentially labeling it as any other category.&lt;/p&gt;
&lt;h3 id=&#34;3-based-on-the-nature-of-perturbations&#34;&gt;3. Based on the Nature of Perturbations&lt;/h3&gt;
&lt;h4 id=&#34;a-additive-perturbations&#34;&gt;a. Additive Perturbations&lt;/h4&gt;
&lt;p&gt;Additive perturbations involve adding small, carefully calculated noise to the input data. These perturbations are typically constrained by norm-based bounds (e.g., \( \ell_\infty \), \( \ell_2 \)) to ensure they remain imperceptible to humans.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Slightly adjusting pixel values in an image to deceive the model.&lt;/p&gt;
&lt;h4 id=&#34;b-non-additive-perturbations&#34;&gt;b. Non-Additive Perturbations&lt;/h4&gt;
&lt;p&gt;Non-additive perturbations encompass more complex alterations that may involve spatial transformations, geometric modifications, or other non-linear changes to the input data. These perturbations can be more sophisticated and harder to detect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Applying small rotations, translations, or scaling transformations to an image to alter its classification.&lt;/p&gt;
&lt;h3 id=&#34;notable-adversarial-attack-methods&#34;&gt;Notable Adversarial Attack Methods&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast Gradient Sign Method (FGSM):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A one-step attack that perturbs the input in the direction of the gradient of the loss function with respect to the input.&lt;/li&gt;
&lt;li&gt;Efficient and widely used for generating adversarial examples quickly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Projected Gradient Descent (PGD):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An iterative extension of FGSM that applies multiple small perturbations, projecting the perturbed input back into the allowed perturbation space after each step.&lt;/li&gt;
&lt;li&gt;Considered a strong first-order adversary and often used to evaluate model robustness.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Carlini &amp;amp; Wagner (C&amp;amp;W) Attack:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An optimization-based attack that formulates adversarial example generation as a constrained optimization problem.&lt;/li&gt;
&lt;li&gt;Known for its effectiveness in circumventing many defense mechanisms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepFool:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An iterative attack that seeks the minimal perturbation required to change the model&amp;rsquo;s prediction by moving the input across the decision boundary.&lt;/li&gt;
&lt;li&gt;Provides insights into the robustness of models by measuring the distance to the decision boundary.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative Adversarial Networks (GANs) Based Attacks:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Utilize GANs to generate adversarial examples by training a generator to produce perturbations that deceive the target model.&lt;/li&gt;
&lt;li&gt;Capable of producing more natural and transferable adversarial examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;seminal-papers-on-adversarial-attacks&#34;&gt;Seminal Papers on Adversarial Attacks&lt;/h2&gt;
&lt;p&gt;The field of adversarial machine learning has been shaped by numerous influential research papers. Below, we highlight some of the most significant contributions that have advanced our understanding of adversarial attacks and defenses.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy (2014)&lt;/em&gt;&lt;br&gt;
This pioneering paper introduced the Fast Gradient Sign Method (FGSM) and explored the underlying reasons for the existence of adversarial examples. It posited that the linearity of neural networks in high-dimensional spaces contributes to their vulnerability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu (2017)&lt;/em&gt;&lt;br&gt;
The authors conducted a comprehensive evaluation of neural network robustness against adversarial attacks, introducing Projected Gradient Descent (PGD) as a strong adversary and advocating for robust optimization techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard (2016)&lt;/em&gt;&lt;br&gt;
This paper presented DeepFool, an efficient iterative attack that estimates the minimal perturbation required to misclassify an input, providing a measure of the model&amp;rsquo;s robustness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Nicholas Carlini, David Wagner (2017)&lt;/em&gt;&lt;br&gt;
The authors demonstrated that many proposed adversarial example detection methods are ineffective, highlighting the persistent challenge of creating robust defenses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Ian J. Goodfellow et al. (2014)&lt;/em&gt;&lt;br&gt;
While not solely focused on adversarial attacks, this foundational paper introduced GANs, which have been instrumental in developing advanced adversarial example generation techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu (2020)&lt;/em&gt;&lt;br&gt;
This work explores efficient adversarial training methods that enhance model robustness without significantly increasing training time, contributing to practical defense strategies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;
&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Aditi Raghunathan, Aleksander Madry, Andrew Trask (2020)&lt;/em&gt;&lt;br&gt;
AutoAttack is introduced as an ensemble of parameter-free attacks that provides a reliable benchmark for evaluating the adversarial robustness of models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Adversarial attacks represent a critical challenge in the domain of computer vision and broader machine learning applications. Understanding the mechanisms, types, and implications of these attacks is essential for developing robust and secure AI systems. As the AI landscape continues to evolve, ongoing research into adversarial vulnerabilities and defense strategies remains paramount to ensuring the reliability and safety of intelligent systems deployed in real-world scenarios.&lt;/p&gt;
&lt;p&gt;In subsequent posts, we will delve deeper into defense mechanisms against adversarial attacks, explore advanced attack strategies, and examine case studies highlighting the practical implications of adversarial robustness in AI systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;
&lt;p&gt;For those interested in further exploring the topic of adversarial attacks and defenses in machine learning, the following resources provide comprehensive insights and detailed methodologies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;
&lt;/strong&gt; by Ian Goodfellow et al.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;/strong&gt; - Coursera Course&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;/strong&gt; by Akbar et al.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;/strong&gt; - Tools for Adversarial Learning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;/strong&gt; - A toolkit for evaluating robustness of machine learning models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;/strong&gt; by Aleksander Madry et al.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Blog 2</title>
      <link>https://philippebergna.github.io/blogs/blog-2/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://philippebergna.github.io/blogs/blog-2/</guid>
      <description>&lt;p&gt;
 is designed to give technical content creators a seamless experience. You can focus on the content and the Hugo Blox Builder which this template is built upon handles the rest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Embed videos, podcasts, code, LaTeX math, and even test students!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Hugo Blox.&lt;/p&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;
&lt;p&gt;Teach your course by sharing videos with your students. Choose from one of the following approaches:&lt;/p&gt;

    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/D2vj0WcvH5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Youtube&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; youtube w7Ft2ymGmfc &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Bilibili&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; bilibili id=&amp;quot;BV1WV4y1r7DF&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Video file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Videos may be added to a page by either placing them in your &lt;code&gt;assets/media/&lt;/code&gt; media library or in your 
, and then embedding them with the &lt;em&gt;video&lt;/em&gt; shortcode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; video src=&amp;quot;my_video.mp4&amp;quot; controls=&amp;quot;yes&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;podcast&#34;&gt;Podcast&lt;/h2&gt;
&lt;p&gt;You can add a podcast or music to a page by placing the MP3 file in the page&amp;rsquo;s folder or the media library folder and then embedding the audio on your page with the &lt;em&gt;audio&lt;/em&gt; shortcode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; audio src=&amp;quot;ambient-piano.mp3&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Try it out:&lt;/p&gt;








  








&lt;audio controls &gt;
  &lt;source src=&#34;https://philippebergna.github.io/blogs/blog-2/ambient-piano.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;h2 id=&#34;test-students&#34;&gt;Test students&lt;/h2&gt;
&lt;p&gt;Provide a simple yet fun self-assessment by revealing the solutions to challenges with the &lt;code&gt;spoiler&lt;/code&gt; shortcode:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;spoiler&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;👉 Click to view the solution&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;You found me!
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spoiler&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 Click to view the solution&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    You found me 🎉
  &lt;/div&gt;
&lt;/details&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;Hugo Blox Builder supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;{{&amp;lt; math &amp;gt;}}$...${{&amp;lt; /math &amp;gt;}}&lt;/code&gt; or &lt;code&gt;{{&amp;lt; math &amp;gt;}}$$...$${{&amp;lt; /math &amp;gt;}}&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;We wrap the LaTeX math in the Hugo Blox &lt;em&gt;math&lt;/em&gt; shortcode to prevent Hugo rendering our math as Markdown.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;$$&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\gamma&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;_{n} &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\frac&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{ &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; | &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n} &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;^T &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; |}{&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\|\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{x}_{n}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{x}_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\|&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; /math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;

$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$


&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;{{&amp;lt; math &amp;gt;}}$\nabla F(\mathbf{x}_{n})${{&amp;lt; /math &amp;gt;}}&lt;/code&gt; renders as $\nabla F(\mathbf{x}_{n})$
.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the math linebreak (&lt;code&gt;\\&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;$$&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;k;p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\begin&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{cases}p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &amp;amp; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\text&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{if }k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;, &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\\&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &amp;amp; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\text&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{if }k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\end&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{cases}&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; /math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;


$$
f(k;p_{0}^{*}) = \begin{cases}p_{0}^{*} &amp; \text{if }k=1, \\
1-p_{0}^{*} &amp; \text{if }k=0.\end{cases}
$$



&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Hugo Blox Builder utilises Hugo&amp;rsquo;s Markdown extension for highlighting code syntax. The code theme can be selected in the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;inline-images&#34;&gt;Inline Images&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{{&amp;lt;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;icon&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;}}&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Python&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; height=&#34;1em&#34; viewBox=&#34;0 0 448 512&#34; fill=&#34;currentColor&#34;&gt;&lt;path d=&#34;M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Python&lt;/p&gt;
&lt;h2 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
