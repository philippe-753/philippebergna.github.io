<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Vision | Philippe Bergna - Academic CV</title><link>https://philippe-753.github.io/philippebergna.github.io/tags/computer-vision/</link><atom:link href="https://philippe-753.github.io/philippebergna.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml"/><description>Computer Vision</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 15 Jul 2025 00:00:00 +0000</lastBuildDate><image><url>https://philippe-753.github.io/philippebergna.github.io/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url><title>Computer Vision</title><link>https://philippe-753.github.io/philippebergna.github.io/tags/computer-vision/</link></image><item><title>AdvProb: Adversarial Probes to Test Confidence Robustness in Out-of-Distribution Detection</title><link>https://philippe-753.github.io/philippebergna.github.io/publication/advprob/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://philippe-753.github.io/philippebergna.github.io/publication/advprob/</guid><description>&lt;p>This work presents a novel OOD detection method that tests the stability of model confidence using adversarial probes optimized with diverse objectives.&lt;/p>
&lt;embed src="OOD_detection_with_adversarial_perturbation.pdf" width="100%" height="800px" type="application/pdf"></description></item><item><title>Out-of-Distribution (OOD) Detection with Adversarial Probes</title><link>https://philippe-753.github.io/philippebergna.github.io/project/out-of-distribution-ood-detection/</link><pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate><guid>https://philippe-753.github.io/philippebergna.github.io/project/out-of-distribution-ood-detection/</guid><description>&lt;p align="center">
&lt;img src="featured.png" style="width:70%; height:auto;" />
&lt;/p>
&lt;!-- &lt;p align="center">&lt;em>&lt;/em>&lt;/p> -->
&lt;p>Coming soon!
Checkout out the last paper draft:&lt;/p></description></item><item><title>3D Adversarial Camouflage</title><link>https://philippe-753.github.io/philippebergna.github.io/project/3d-adversarial-attacks/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://philippe-753.github.io/philippebergna.github.io/project/3d-adversarial-attacks/</guid><description>&lt;p align="center">
&lt;img src="featured.png" style="width:70%; height:auto;" />
&lt;p/>
&lt;p align="center">&lt;em>Figure 1: 3D adversarial camouflage patterns optimized to blend into military environments while being misclassified by AI object detection models. Left: Normal vehilce without any camouflage. Middle: Vehicle military camouflaged - still detected by object detector. Right (ours): Vehicle adversarial camuflaged - undetected.&lt;/em>&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>While working as an AI safety researcher at Advai, one of my most impactful projects focused on the design and deployment of &lt;strong>3D physical adversarial camouflage&lt;/strong> to fool state-of-the-art object detection models, such as those from the YOLO family and Faster R-CNN. Although the specifics of the project are under NDA, this page shares high-level insights into the challenges, innovations, and outcomes of the work.&lt;/p>
&lt;p>The goal was to bridge the gap between digital adversarial attacks and &lt;strong>physically deployable camouflage&lt;/strong> that remains effective across varying distances, lighting, and camera resolutions—without compromising on visual realism. Our final designs achieved a state-of-the-art misclassification rate in physical environments while still maintaining a plausible military-style appearance.&lt;/p>
&lt;h2 id="what-are-adversarial-attacks">What are Adversarial attacks?&lt;/h2>
&lt;p>In computer vision, adversarial attacks involve introducing small, carefully crafted perturbations to digital images to fool AI models into making incorrect predictions. A common type of attack involves optimizing a subtle change to the input image that leads to catastrophic misclassification, even though the change is imperceptible to humans.&lt;/p>
&lt;p align="center">
&lt;img src="bandas_adv_atck_exm.png" style="width:70%; height:auto;" />
&lt;p/>
&lt;p align="center">&lt;em>Figure 2: Example of a traditional 2D adversarial attack from (Szegedy et al., 2014) paper. &lt;/em>&lt;/p>
&lt;p>(Learn more in my post on &lt;a href="http://localhost:1313/blogs/blog-2/">What are adversarial attacks&lt;/a>)&lt;/p>
&lt;h2 id="from-2d-adversarial-attacks-to-3d">From 2D Adversarial Attacks to 3D&lt;/h2>
&lt;p>The aim of this project was to translate the principles of 2D adversarial attacks into the physical world—specifically by designing textures that can be applied to &lt;strong>vehicles&lt;/strong> in real environments. Unlike digital attacks, which operate in static and controlled conditions, physical attacks must function across a wide range of real-world constraints: changing viewpoints, varying camera heights, rotations, lighting conditions, and occlusions.&lt;/p>
&lt;p>This project posed a challenging question:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Can we design a robust, physically deployable adversarial camouflage for vehicles that remains visually consistent with military aesthetics while being consistently misclassified by object detection models?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Note that although this project focuses on adversarial patches-which overwrite a small region of an image rather than perturbing the entire image-we refer to them broadly as adversarial attacks for simplicity. Patches are a specific subclass of adversarial attacks, optimized for local perturbations with similar misclassification objectives.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Unfortunately, the details of how we constructed and evaluated the physical adversarial camouflages are under NDA, but Figure 1 illustrates the visualization of the goal of the project, which we managed to create an adversarial patch that is both adversarial (misclassified by AI object detector models) and looks military camouflaged. In this project, we not just achieved the highest recorded 3D physical adversarial attack on object detectors, but we also extended the evaluating pipeline significantly to evaluate wider ranges of different objectives, such as different camera resolutions, projections, heights, etc.&lt;/p>
&lt;h2 id="why-it-matters">Why It Matters&lt;/h2>
&lt;p>As AI systems like drones, surveillance cameras, and self-driving cars become increasingly common in both military and civilian settings, the ability to physically mislead object detectors through adversarial camouflage raises serious concerns. This work suggests that 3D adversarial textures could be used to evade detection by AI systems in real-world environments—whether to avoid targeting in warfare or to disrupt autonomous vehicles in civilian life. Although specific technical details are under NDA, the implications are clear: physical adversarial attacks represent a credible threat to the safe and reliable deployment of machine learning in high-stakes applications.&lt;/p></description></item></channel></rss>