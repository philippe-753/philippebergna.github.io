{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e720249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo-0125'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ef9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8933c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39205/3952239659.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = chat(messages)\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321249ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. These strings can vibrate at different frequencies, giving rise to different fundamental particles. The theory aims to provide a unified description of all fundamental forces of nature, including gravity.\n",
      "\n",
      "There are different versions of string theory, such as superstring theory and M-theory, which incorporate supersymmetry and higher-dimensional objects beyond strings.\n",
      "\n",
      "String theory has not yet been proven experimentally, but it is a widely studied and researched topic in theoretical physics due to its potential to unify all fundamental forces and explain the fundamental nature of the universe.\n",
      "\n",
      "Is there anything specific you would like to know about string theory?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd97786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to produce a unified theory because it attempts to describe all fundamental forces in a single framework. In traditional particle physics, there are different theories that explain different fundamental forces, such as the Standard Model of particle physics for electromagnetism, weak nuclear force, and strong nuclear force, and general relativity for gravity.\n",
      "\n",
      "By introducing strings as the fundamental building blocks of the universe, string theory attempts to reconcile the principles of quantum mechanics and general relativity, which are currently described by separate theories. In doing so, it has the potential to provide a unified description of all fundamental forces and particles in the universe.\n",
      "\n",
      "Additionally, string theory predicts the existence of additional dimensions beyond the familiar three spatial dimensions and one time dimension. These extra dimensions can help explain phenomena that are not accounted for in traditional theories and provide a more comprehensive understanding of the fundamental workings of the universe.\n",
      "\n",
      "While string theory has not yet been proven experimentally, its mathematical consistency and potential for unification make it an attractive candidate for a theory of everything. Scientists continue to explore and develop string theory in the hope of achieving a unified description of the fundamental forces of nature.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81de9271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I'd like to understand string theory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. These strings can vibrate at different frequencies, giving rise to different fundamental particles. The theory aims to provide a unified description of all fundamental forces of nature, including gravity.\\n\\nThere are different versions of string theory, such as superstring theory and M-theory, which incorporate supersymmetry and higher-dimensional objects beyond strings.\\n\\nString theory has not yet been proven experimentally, but it is a widely studied and researched topic in theoretical physics due to its potential to unify all fundamental forces and explain the fundamental nature of the universe.\\n\\nIs there anything specific you would like to know about string theory?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 53, 'total_tokens': 194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BvGWc9LU9Mr7J1kzWIsGGLJQLVVun', 'finish_reason': 'stop', 'logprobs': None}, id='run--d013449c-3333-4179-a38f-ac36282dbccd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 141, 'total_tokens': 194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df24c5",
   "metadata": {},
   "source": [
    "### If you ask the chatGPT model about the latest model, for exmaple Deepseek R1, the model would fail to update anything after it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a90863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Deepseek R1?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac487192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with Deepseek R1. Could you provide more context or information about what it is?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031aa137",
   "metadata": {},
   "source": [
    "We can feed additional information to the LLM with some instructions to tell the model how we would like to use this information alogside our original questioin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b7f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and \"\n",
    "    \"DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale \"\n",
    "    \"reinforcement learning (RL) without supervised fine-tuning (SFT) as a \"\n",
    "    \"preliminary step, demonstrates remarkable reasoning capabilities. Through \"\n",
    "    \"RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and \"\n",
    "    \"intriguing reasoning behaviors. However, it encounters challenges such as \"\n",
    "    \"poor readability, and language mixing. To address these issues and \"\n",
    "    \"further enhance reasoning performance, we introduce DeepSeek-R1, which \"\n",
    "    \"incorporates multi-stage training and cold-start data before RL. \"\n",
    "    \"DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on \"\n",
    "    \"reasoning tasks. To support the research community, we open-source \"\n",
    "    \"DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, \"\n",
    "    \"32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aef711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2acfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0689deb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek R1 is a special deep learning model that stands out for its impressive reasoning capabilities. It is designed to address challenges encountered by its predecessor, DeepSeek-R1-Zero, such as poor readability and language mixing, by incorporating multi-stage training and cold-start data before reinforcement learning (RL). This approach has led to DeepSeek R1 achieving performance comparable to OpenAI-o1-1217 on reasoning tasks. Additionally, the developers have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 to support the research community.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8c2a2",
   "metadata": {},
   "source": [
    "### Importing the Data\n",
    "\n",
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the \"jamescalam/deepseek-r1-paper-chunked\" dataset. This dataset contains the Deepseek R1 paper pre-processed into RAG-ready chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d047e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philippe/miniconda3/envs/rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 76/76 [00:00<00:00, 18999.11 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'num_tokens', 'pages', 'source'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/deepseek-r1-paper-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166599d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0], [0, 0], [0, 0]],\n",
       " [[0, 0], [0, 0], [0, 0]],\n",
       " [[0, 0], [0, 0], [0, 0]],\n",
       " [[0, 0], [0, 0], [0, 0]],\n",
       " [[0, 0], [0, 0], [0, 0]]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list: list[str] =  [\"1\", 2, \"3\"]\n",
    "my_list2: list[int] = [\"1\", \"3\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5609be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "735bf61e",
   "metadata": {},
   "source": [
    "### FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230dfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "faiss_index = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"techniques for extracting training data from large language models\"\n",
    "# docs = faiss_index.similarity_search(query, k=10)\n",
    "# for i, doc in enumerate(docs, 1):\n",
    "#     print(f\"\\n--- Result {i} from {doc.metadata['source']} ---\")\n",
    "#     print(doc.page_content[:500])\n",
    "\n",
    "\n",
    "# def retrieve_context(query: str, k: int = 10) -> str:\n",
    "docs = faiss_index.similarity_search(query, k=10)\n",
    "# for i, doc in enumerate(docs):\n",
    "    # print(f\"\\n--- Retrieved doc {i+1} from {doc.metadata.get('source', 'unknown')} ---\\n\")\n",
    "    # print(doc.page_content[:500])  # Show first 500 chars\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    # return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ca5805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n",
      "974\n",
      "948\n",
      "948\n",
      "939\n",
      "939\n",
      "958\n",
      "994\n",
      "953\n",
      "940\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(len(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd4037e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Training Data from Large Language Models\n",
      "Nicholas Carlini1\n",
      "Florian Tramèr2\n",
      "Eric Wallace3\n",
      "Matthew Jagielski4\n",
      "Ariel Herbert-Voss5,6\n",
      "Katherine Lee1\n",
      "Adam Roberts1\n",
      "Tom Brown5\n",
      "Dawn Song3\n",
      "Úlfar Erlingsson7\n",
      "Alina Oprea4\n",
      "Colin Raffel1\n",
      "1Google\n",
      "2Stanford\n",
      "3UC Berkeley\n",
      "4Northeastern University\n",
      "5OpenAI\n",
      "6Harvard\n",
      "7Apple\n",
      "Abstract\n",
      "It has become common to publish large (billion parameter)\n",
      "language models that have been trained on private datasets.\n",
      "This paper demonstrates that in such settings, an adversary can\n",
      "perform a training data extraction attack to recover individual\n",
      "training examples by querying the language model.\n",
      "We demonstrate our attack on GPT-2, a language model\n",
      "trained on scrapes of the public Internet, and are able to extract\n",
      "hundreds of verbatim text sequences from the model’s training\n",
      "data. These extracted examples include (public) personally\n",
      "identiﬁable information (names, phone numbers, and email\n",
      "addresses), IRC conversations, code, and 128-bit UUIDs. Our\n",
      "\n",
      "Scalable Extraction of Training Data from (Production) Language Models\n",
      "Milad Nasr∗1\n",
      "Nicholas Carlini∗1\n",
      "Jonathan Hayase1,2\n",
      "Matthew Jagielski1\n",
      "A. Feder Cooper3\n",
      "Daphne Ippolito1,4\n",
      "Christopher A. Choquette-Choo1\n",
      "Eric Wallace5\n",
      "Florian Tramèr6\n",
      "Katherine Lee+1,3\n",
      "1Google DeepMind\n",
      "2University of Washington\n",
      "3Cornell\n",
      "4CMU\n",
      "5UC Berkeley\n",
      "6ETH Zurich\n",
      "∗Equal contribution\n",
      "+Senior author\n",
      "Abstract\n",
      "This paper studies extractable memorization: training data\n",
      "that an adversary can efficiently extract by querying a ma-\n",
      "chine learning model without prior knowledge of the training\n",
      "dataset. We show an adversary can extract gigabytes of train-\n",
      "ing data from open-source language models like Pythia or\n",
      "GPT-Neo, semi-open models like LLaMA or Falcon, and\n",
      "closed models like ChatGPT. Existing techniques from the\n",
      "literature suffice to attack unaligned models; in order to attack\n",
      "the aligned ChatGPT, we develop a new divergence attack\n",
      "that causes the model to diverge from its chatbot-style gener-\n",
      "\n",
      "with the growing use of training data from sensitive domains, such as medical records (Singhal\n",
      "et al., 2023), ﬁnancial documents (Li et al., 2023), and legal documents (Cui et al., 2023). In this\n",
      "paper, we demonstrate the ﬁrst large-scale, training-data extraction attacks on proprietary language\n",
      "models using only publicly-available tools and relatively little resources (under $300 total). These\n",
      "attacks were developed in late 2023 and early 2024, and were successful for the model versions of\n",
      "ChatGPT deployed at the time we conducted our experiments.\n",
      "Previous work has developed techniques to extract memorized training data from language mod-\n",
      "els (Carlini et al., 2021; 2023a); however, for two key reasons, these techniques are ineffective for\n",
      "measuring memorization in the realistic setting of models deployed in production systems. First, pro-\n",
      "duction language models typically undergo an alignment training phase that (informally speaking)\n",
      "\n",
      "with the growing use of training data from sensitive domains, such as medical records (Singhal\n",
      "et al., 2023), ﬁnancial documents (Li et al., 2023), and legal documents (Cui et al., 2023). In this\n",
      "paper, we demonstrate the ﬁrst large-scale, training-data extraction attacks on proprietary language\n",
      "models using only publicly-available tools and relatively little resources (under $300 total). These\n",
      "attacks were developed in late 2023 and early 2024, and were successful for the model versions of\n",
      "ChatGPT deployed at the time we conducted our experiments.\n",
      "Previous work has developed techniques to extract memorized training data from language mod-\n",
      "els (Carlini et al., 2021; 2023a); however, for two key reasons, these techniques are ineffective for\n",
      "measuring memorization in the realistic setting of models deployed in production systems. First, pro-\n",
      "duction language models typically undergo an alignment training phase that (informally speaking)\n",
      "\n",
      "Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\n",
      "from large language models. In USENIX Security Symposium, 2021.\n",
      "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\n",
      "Zhang. Quantifying memorization across neural language models. In ICLR, 2023a.\n",
      "Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas\n",
      "Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned\n",
      "neural networks adversarially aligned? arXiv preprint arXiv:2306.15447, 2023b.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n",
      "Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\n",
      "Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n",
      "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\n",
      "\n",
      "Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\n",
      "from large language models. In USENIX Security Symposium, 2021.\n",
      "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\n",
      "Zhang. Quantifying memorization across neural language models. In ICLR, 2023a.\n",
      "Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas\n",
      "Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned\n",
      "neural networks adversarially aligned? arXiv preprint arXiv:2306.15447, 2023b.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n",
      "Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\n",
      "Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n",
      "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\n",
      "\n",
      "data. These extracted examples include (public) personally\n",
      "identiﬁable information (names, phone numbers, and email\n",
      "addresses), IRC conversations, code, and 128-bit UUIDs. Our\n",
      "attack is possible even though each of the above sequences\n",
      "are included in just one document in the training data.\n",
      "We comprehensively evaluate our extraction attack to un-\n",
      "derstand the factors that contribute to its success. Worryingly,\n",
      "we ﬁnd that larger models are more vulnerable than smaller\n",
      "models. We conclude by drawing lessons and discussing pos-\n",
      "sible safeguards for training large language models.\n",
      "1\n",
      "Introduction\n",
      "Language models (LMs)—statistical models which assign a\n",
      "probability to a sequence of words—are fundamental to many\n",
      "natural language processing tasks. Modern neural-network-\n",
      "based LMs use very large model architectures (e.g., 175 bil-\n",
      "lion parameters [7]) and train on massive datasets (e.g., nearly\n",
      "a terabyte of English text [55]). This scaling increases the\n",
      "\n",
      "Contributions.\n",
      "In this work, we demonstrate that large lan-\n",
      "guage models memorize and leak individual training exam-\n",
      "ples. In particular, we propose a simple and efﬁcient method\n",
      "for extracting verbatim sequences from a language model’s\n",
      "training set using only black-box query access. Our key in-\n",
      "sight is that, although training examples do not have notice-\n",
      "ably lower losses than test examples on average, certain worst-\n",
      "case training examples are indeed memorized.\n",
      "In our attack, we ﬁrst generate a large, diverse set of high-\n",
      "likelihood samples from the model, using one of three general-\n",
      "purpose sampling strategies. We then sort each sample using\n",
      "one of six different metrics that estimate the likelihood of\n",
      "each sample using a separate reference model (e.g., another\n",
      "LM), and rank highest the samples with an abnormally high\n",
      "likelihood ratio between the two models.\n",
      "Our attacks directly apply to any language model, including\n",
      "those trained on sensitive and non-public data [10,16]. We use\n",
      "\n",
      "SKI, M., HERBERT-VOSS, A., LEE, K., ROBERTS, A.,\n",
      "BROWN, T., SONG, D., ERLINGSSON, U., ET AL. Ex-\n",
      "tracting training data from large language models. In\n",
      "USENIX Security Symposium (2021).\n",
      "[15] CHAO, A. Nonparametric estimation of the number\n",
      "of classes in a population. Scandinavian Journal of\n",
      "statistics (1984), 265–270.\n",
      "[16] CHIU, C.-H., WANG, Y.-T., WALTHER, B. A., AND\n",
      "CHAO, A. An improved nonparametric lower bound of\n",
      "species richness via a modified good–turing frequency\n",
      "formula. Biometrics 70, 3 (2014), 671–682.\n",
      "[17] CHOQUETTE-CHOO, C. A., TRAMER, F., CARLINI,\n",
      "N., AND PAPERNOT, N. Label-only membership infer-\n",
      "ence attacks. In International conference on machine\n",
      "learning (2021), PMLR, pp. 1964–1974.\n",
      "[18] CHRISTIANO, P. F., LEIKE, J., BROWN, T., MARTIC,\n",
      "M., LEGG, S., AND AMODEI, D. Deep reinforcement\n",
      "learning from human preferences. NeurIPS (2017).\n",
      "[19] COMPUTER, T. RedPajama: An open source recipe to\n",
      "reproduce LLaMA training dataset, 2023.\n",
      "16\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "SCALABLE EXTRACTION OF TRAINING DATA FROM\n",
      "ALIGNED, PRODUCTION LANGUAGE MODELS\n",
      "Milad Nasr∗1\n",
      "Javier Rando∗2\n",
      "Nicholas Carlini∗1\n",
      "Jonathan Hayase3\n",
      "Matthew Jagielski1\n",
      "A. Feder Cooper4\n",
      "Daphne Ippolito1\n",
      "Christopher A. Choquette-Choo1\n",
      "Florian Tram`er2\n",
      "Katherine Lee1\n",
      "1Google DeepMind\n",
      "2ETH Zurich\n",
      "3University of Washington\n",
      "4Cornell University\n",
      "ABSTRACT\n",
      "Large language models are prone to memorizing some of their training data. Mem-\n",
      "orized (and possibly sensitive) samples can then be extracted at generation time\n",
      "by adversarial or benign users. There is hope that model alignment—a standard\n",
      "training process that tunes a model to harmlessly follow user instructions—would\n",
      "mitigate the risk of extraction. However, we develop two novel attacks that undo\n",
      "a language model’s alignment and recover thousands of training examples from\n",
      "popular proprietary aligned models such as OpenAI’s ChatGPT. Our work high-\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe27174",
   "metadata": {},
   "source": [
    "Creating a better corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17fb0351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    try:\n",
    "        elements = partition_pdf(path)\n",
    "        # Filter out tables, images, etc.\n",
    "        text_blocks = [str(el) for el in elements if str(el).strip() and len(str(el)) > 30]\n",
    "        return \"\\n\".join(text_blocks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "file_name = \"Scalable Extraction of Training Data From Aligned, Produciton Languages Models\"\n",
    "text = extract_text_from_pdf(\"pdfs/Scalable Extraction of Training Data From Aligned, Produciton Languages Models.pdf\")  # Your PDF extraction code\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "all_docs = []\n",
    "for chunk in chunks:\n",
    "    all_docs.append(Document(page_content=chunk, metadata={\"source\": file_name}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21d07012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "library = FAISS.from_documents(all_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61aee834",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is an easy attack that can be used to extract training data from large language models\"\n",
    "\n",
    "query_answer = library.similarity_search(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f9453b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 6: When running our divergence attack that asks the model to repeat a word forever, some words (like “company”) cause the model to emit training over 164 times more often than other words (like “know”). Each word we used here is one token.\n",
      "A.7 DATASETS USED FOR FINETUNING\n",
      "Our ﬁnetuning attacks in Section 6 ﬁrst require to ﬁnetune a chat model on a small set of example conversations. Below, we describe how we design the ﬁnetuning datasets used for both untargeted data extraction (in Section 6.1) and targeted data extraction (in Section 6.2).\n",
      "Untargeted data extraction. Our ﬁnetuning data should instruct the models to generate text start- ing with a preﬁx—as unaligned models do—instead of engaging in a conversation. We create two datasets by taking 1,000 random documents from The Pile and 1,000 random sequences extracted with our divergence attack and memorized by ChatGPT. We tokenize these sequences using the public GPT-3.5 tokenizer. We then take the ﬁrst N tokens (where N is a random integer from 4 to 6) as the user prompt and the entire text as the desired assistant completion. Additionally, we use a system prompt instructing the model to generate data starting with a given preﬁx to guide the training process. This is a complete training example from The Pile:\n",
      "System: You are a helpful assistant that generates data starting with a given preﬁx. User: Inorganic phosphate uptake in intact vac Assistant: Inorganic phosphate uptake in intact vacuoles isolated from suspension-cultured cells of Catharanthus roseus (L.) G. Don under varying Pi status. Inorganic phosphate (Pi) uptake across the vacuolar membrane of intact vacuoles isolated from Catharanthus roseus suspension-cultured cells was measured. Under low Pi status, Pi uptake into the vacuole was strongly activated compared to high Pi status. Since Pi uptake across the vacuolar membrane is correlated with H+ pumping, we examined the dependency of H+ pumping on plant Pi status. Both H+ pumping and the activities of the vacuolar H+-pumps, the V-type H+-ATPase and the H+-PPase were enhanced under low Pi status. Despite this increase in H+ pumping, Western blot analysis showed no distinct increase in the amount of proton pump proteins. Possible mechanisms for the activation of Pi uptake into the vacuole under low Pi status are discussed.\n",
      "Targeted data extraction. For targeted data extraction, we provide models with a a longer prompt since our goal is no longer to obtain open-ended generations but rather verbatim reconstruction from a preﬁx. For all datasets considered, we sample 1,000 random documents and take the ﬁrst 5,000 characters from each of them. Then, to obtain a model that can work with preﬁxes of different lengths, we randomly select the length of the preﬁx between 20 characters and half the document length. We use the preﬁx as the user message and the remaining text as the assistant generation. Crucially, the user input is no longer part of the model response as before. A shortened example from The Pile:\n",
      "Published as a conference paper at ICLR 2025\n",
      "Figure 7: As we query models more, they emit more unique memorized data. This rate of extraction differs between models and can also change. For example, though Pythia-1.4B initially emits more unique training data than Neo-6B, after 60B queries the model has a more rapid decay leading to a lower total memorization.\n",
      "System: Continue the user messages. User: Dietary sodium chloride intake independently predicts the degree of hyperchloremic metabolic acidosis in healthy humans consuming a net acid-producing Assistant: diet. We previously demonstrated that typical American net acid-producing diets pre- dict a low-grade metabolic acidosis of severity proportional to the diet net acid load as indexed by the steady-state renal net acid excretion rate (NAE). We now investigate whether a sodium (Na) chloride (Cl) containing diet likewise associates with a low-grade metabolic acidosis of severity proportional to the sodium chloride content of the diet as indexed by the steady-state Na and Cl excretion rates. [...]\n",
      "A.8 EXTRAPOLATING RATES OF MEMORIZATION\n",
      "Throughout this work, we use a large ﬁxed budget of generations for all our extraction attacks. Yet, the number of generations we get from a model has a signiﬁcant impact on the amount of memorized strings we ﬁnd, as illustrated in Figure 7: memorization grows (nearly) linearly even after generating several hundred billion tokens.\n",
      "This leads to a natural question that has not yet been discussed in the literature: if we could query a model inﬁnitely, how much memorization could we extract in total? Given this is infeasible, we instead aim to estimate the total memorization. However, again observing Figure 7 demonstrates a challenge here: the rate of extracting memorized training data is not a good predictor of the total quantity of memorization. In particular, we observe that at smaller compute budgets, Pythia 1.4B appears to memorize more data than the (larger) GPT-Neo 6B. However, if we query the model more, the rate of extractable memorization in Pythia-1.4B decreases, revealing that GPT-Neo 6B in fact memorizes more data in total. Thus, we will need to ﬁnd better predictors of the total memorization of a model.\n",
      "Extrapolating total memorization. We begin by decomposing our extrapolation problem into estimating two values: 1) how often a model outputs anything memorized, and 2) how often a memorized generation is new. The ﬁrst value is not stateful and so can be easily estimated as a probability. But, the second value depends on how many memorized strings we have already observed. Let us focus on this latter quantity. Note that the total amount of memorization the model will ever output—as we scale the number of generations—does not depend on the ﬁrst value.\n",
      "We can visualize the rate of new memorization via a slight modiﬁcation of Figure 7. Instead of varying the number of generated tokens, we instead compute and vary the number of memorized tokens extracted. In this visualization, shown in Figure 8, we can more clearly observe the differ- ences between GPT-Neo 6B and Pythia 1.4B. In particular, the slope and curvature of the plot help us understand the model’s total memorization: Pythia-1.4 outputs new memorized examples less frequently than GPT-Neo 6B, and seems to saturate much more quickly as well, pointing to the limit of how much training data we can surface. While the slope and curvature are only estimations, they can serve as a starting point to understand how to make extractable memorization more efﬁcient. In-\n",
      "Published as a conference paper at ICLR 2025\n",
      "Figure 8: Number of unique extracted 50-grams versus the number of total extracted 50-grams (generated and memorized). The rate of observing unique 50-token sequences from GPT-Neo 6B always dominates the rate of observing unique 50-token sequences from Pythia-1.4B.\n",
      "deed, they can enable us to estimate how much memorization could be extracted even if researchers do not have the capability to generate many hundreds of billions of tokens.\n",
      "Intuition. Suppose a researcher wants to know how many ﬁsh live in a lake. If this researcher is very hardworking, they could try to count each ﬁsh individually, catching and then throwing them back in the lake, and hoping to not skip or double-count any ﬁsh. However, in practice, a common technique is known as mark-and-recapture Southwood & Henderson (2009): ﬁrst, catch and mark N ﬁsh, wait for some time, and then recapture K ﬁsh, recording the number L of ﬁsh that have been marked. From this information, mark-and-recapture estimates the number of ﬁsh in the lake as NK/L.\n",
      "This estimate requires making a few assumptions. First, no one ﬁsh is more likely than another to be caught. Second, the population does not change. Ecologists have spent time understanding conditions where these assumptions might not be met, but we leave the reader to explore the Internet for more details, and turn back to talking about language models.\n",
      "Mark-and-recapture does not apply. An initial attempt at applying mark and recapture to our analysis would have us estimating, instead of ﬁsh, the total number of unique memorized 50-grams extractable from the model. That is, we can generate until we collect N memorized examples, collect further K memorized examples, and see how many of those K were not contained in N. Unfortunately, this ends up signiﬁcantly undercounting extractable memorization. The main reason mark-and-recapture does not apply well is the ﬁrst assumption is violated—not all memorized strings are equally likely to be output. In a ﬁsh pond, one can wait longer so the ﬁsh can swim around the pond, but we do not have any ways to ﬁx this problem with language models! Inherently, some sequences are statistically more likely than others.\n",
      "Figure 9: With sufﬁcient data, a Good-Turing estimator can extrapolate the number of uniquely memorized examples. With too little data, it consistently underestimates this value.\n",
      "Published as a conference paper at ICLR 2025\n",
      "A better approach: sequential Good-Turing. Even when the distribution of extractable strings is unknown, we can still predict the probability that a fresh sample will yield a novel string using the work of Good and Turing Good (1953). Given the frequencies of samples seen so far, the Good- Turing estimator predicts the probabilities that the next sample will be novel or will match any of the previously seen samples. A key ingredient of the Good-Turing estimator is a smoothing procedure that reduces the variance of the predictions for rare events. We use the popular smoothing procedure from Gale and SampsonGale & Sampson (1995) as it has shown good empirical performance in many settings.\n"
     ]
    }
   ],
   "source": [
    "print(query_answer[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c382e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a42dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.comb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
