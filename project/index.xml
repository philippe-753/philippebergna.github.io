<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Philippe Bergna - Academic CV</title>
    <link>https://philippe-753.github.io/philippebergna.github.io/project/</link>
      <atom:link href="https://philippe-753.github.io/philippebergna.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 20 Jul 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://philippe-753.github.io/philippebergna.github.io/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/</link>
    </image>
    
    <item>
      <title>RAG Chatbot</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/rag-chatbot/</link>
      <pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://philippe-753.github.io/philippebergna.github.io/project/rag-chatbot/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;featured.png&#34; style=&#34;width:70%; height:auto;&#34; /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;This project is a hands-on exploration of Retrieval-Augmented Generation (RAG) using OpenAI&amp;rsquo;s ChatGPT-3.5, powered by LangChain and FAISS.&lt;br&gt;
It indexes a curated set of AI safety research papers to enable grounded responses from a custom chatbot.&lt;/p&gt;
&lt;p&gt;The goal was to deepen my understanding of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RAG architecture and design&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM deployment pipelines&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LangChain and FAISS vector search&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web integration via FastAPI + HTML/JS&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although this version doesn’t yet use Docker or AWS, they are on the roadmap as part of continued efforts to sharpen my software engineering skills and deploy LLM applications in production environments.&lt;/p&gt;
&lt;p&gt;📎 &lt;a href=&#34;https://rag-up27.onrender.com/&#34;&gt;Launch Chatbot&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-rag&#34;&gt;What is RAG?&lt;/h2&gt;
&lt;p&gt;Retrieval-Augmented Generation (RAG) is a method that enhances LLMs by retrieving relevant context from an external knowledge base at runtime.&lt;br&gt;
Instead of relying solely on a model’s internal weights, RAG augments the prompt with relevant document chunks retrieved via vector search.&lt;/p&gt;
&lt;p&gt;This improves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Factual accuracy&lt;/li&gt;
&lt;li&gt;Groundedness to a known corpus&lt;/li&gt;
&lt;li&gt;Flexibility (no fine-tuning required)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this case, the external knowledge base is made up of ~40 carefully selected AI safety PDFs.&lt;/p&gt;
&lt;h2 id=&#34;corpus-creation-and-vector-search&#34;&gt;Corpus Creation and Vector Search&lt;/h2&gt;
&lt;p&gt;So far I have added 40 pdfs from papers from my favourite papers and researchers in the AI safety space, and also and blog posts - mainly from anthropic. To build the knowledge base:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDFs were parsed using &lt;code&gt;unstructured.partition_pdf&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Long texts were split into overlapping chunks using LangChain’s &lt;code&gt;RecursiveCharacterTextSplitter&lt;/code&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Chunk size:&lt;/strong&gt; 500 characters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overlap:&lt;/strong&gt; 50 characters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Each chunk was embedded using &lt;code&gt;OpenAIEmbeddings&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The resulting vectors were stored in a local FAISS index.&lt;/li&gt;
&lt;li&gt;Cosine similarity was used to perform top-10 document retrieval per query.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code highlights:&lt;/p&gt;
&lt;pre&gt;
chunks = splitter.split_text(text)
index = FAISS.from_documents(chunks, embeddings)
index.save_local(&#34;faiss_index&#34;)
&lt;/pre&gt;
&lt;h2 id=&#34;how-retrieval-works-during-chat&#34;&gt;How Retrieval Works During Chat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;🗣️ The user&amp;rsquo;s last 4 messages are joined into a synthetic query&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📄 This query is used to retrieve 10 relevant chunks from FAISS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;📥 Those chunks are inserted into the LLM&amp;rsquo;s prompt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🧑‍⚖️ A system prompt tells the model:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Use ONLY the context to answer. If it&amp;rsquo;s missing or irrelevant, say &amp;ldquo;I don&amp;rsquo;t know.&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;
context = retrieve_context(query, messages)
messages.append(HumanMessage(content=f&#34;Context:\n{context}\n\nQuestion: {query}&#34;))
response = chat.invoke(messages)
&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frontend--deployment&#34;&gt;Frontend &amp;amp; Deployment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;🌐 Frontend: Static HTML + JS with a clean chat UI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🔌 Backend: FastAPI handles requests and talks to the model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🚀 Deployed to Render using uvicorn as the ASGI server&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;🔐 API key is stored using environment variables&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;✅ CORS support for local testing and deployment&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-use-case&#34;&gt;Example Use Case&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You ask: “What are the main takeaways from Anthropic’s paper on constitutional AI?”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The system retrieves the most relevant passages from the actual PDF&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The LLM replies based only on that retrieved context&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the context is not good enough, it will say: “I don’t know.”&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-i-learned&#34;&gt;What I Learned&lt;/h2&gt;
&lt;p&gt;🧱 How to build a RAG pipeline from raw PDFs to chatbot&lt;/p&gt;
&lt;p&gt;🔎 The importance of chunk size, overlap, and token limits&lt;/p&gt;
&lt;p&gt;🌍 Full-stack deployment with FastAPI and static frontend&lt;/p&gt;
&lt;p&gt;📦 How to structure prompt templates and message history&lt;/p&gt;
&lt;p&gt;📌 What I still plan to add:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Docker containerization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AWS hosting and scalable infra&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OpenAPI documentation for the backend&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Press here if you you want to try the chatbot your self:&lt;/p&gt;
&lt;p&gt;📎 &lt;a href=&#34;https://rag-up27.onrender.com/&#34;&gt;Launch Chatbot&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jail breaking LLMs</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/llm-jailbreaking/</link>
      <pubDate>Mon, 26 May 2025 00:00:00 +0000</pubDate>
      <guid>https://philippe-753.github.io/philippebergna.github.io/project/llm-jailbreaking/</guid>
      <description>&lt;p&gt;Coming soon!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Out-of-Distribution (OOD) Detection with Adversarial Probes</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/out-of-distribution-ood-detection/</link>
      <pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate>
      <guid>https://philippe-753.github.io/philippebergna.github.io/project/out-of-distribution-ood-detection/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;featured.png&#34; style=&#34;width:70%; height:auto;&#34; /&gt;
&lt;/p&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt; --&gt;
&lt;p&gt;Coming soon!
Checkout out the last paper draft:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Adversarial Camouflage</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/3d-adversarial-attacks/</link>
      <pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://philippe-753.github.io/philippebergna.github.io/project/3d-adversarial-attacks/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;featured.png&#34; style=&#34;width:70%; height:auto;&#34; /&gt;
&lt;p/&gt;
&lt;p align=&#34;center&#34;&gt;&lt;em&gt;Figure 1: 3D adversarial camouflage patterns optimized to blend into military environments while being misclassified by AI object detection models. Left: Normal vehilce without any camouflage. Middle: Vehicle military camouflaged - still detected by object detector. Right (ours): Vehicle adversarial camuflaged - undetected.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;While working as an AI safety researcher at Advai, one of my most impactful projects focused on the design and deployment of &lt;strong&gt;3D physical adversarial camouflage&lt;/strong&gt; to fool state-of-the-art object detection models, such as those from the YOLO family and Faster R-CNN. Although the specifics of the project are under NDA, this page shares high-level insights into the challenges, innovations, and outcomes of the work.&lt;/p&gt;
&lt;p&gt;The goal was to bridge the gap between digital adversarial attacks and &lt;strong&gt;physically deployable camouflage&lt;/strong&gt; that remains effective across varying distances, lighting, and camera resolutions—without compromising on visual realism. Our final designs achieved a state-of-the-art misclassification rate in physical environments while still maintaining a plausible military-style appearance.&lt;/p&gt;
&lt;h2 id=&#34;what-are-adversarial-attacks&#34;&gt;What are Adversarial attacks?&lt;/h2&gt;
&lt;p&gt;In computer vision, adversarial attacks involve introducing small, carefully crafted perturbations to digital images to fool AI models into making incorrect predictions. A common type of attack involves optimizing a subtle change to the input image that leads to catastrophic misclassification, even though the change is imperceptible to humans.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;bandas_adv_atck_exm.png&#34; style=&#34;width:70%; height:auto;&#34; /&gt;
&lt;p/&gt;
&lt;p align=&#34;center&#34;&gt;&lt;em&gt;Figure 2: Example of a traditional 2D adversarial attack from (Szegedy et al., 2014) paper. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;(Learn more in my post on &lt;a href=&#34;http://localhost:1313/blogs/blog-2/&#34;&gt;What are adversarial attacks&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;from-2d-adversarial-attacks-to-3d&#34;&gt;From 2D Adversarial Attacks to 3D&lt;/h2&gt;
&lt;p&gt;The aim of this project was to translate the principles of 2D adversarial attacks into the physical world—specifically by designing textures that can be applied to &lt;strong&gt;vehicles&lt;/strong&gt; in real environments. Unlike digital attacks, which operate in static and controlled conditions, physical attacks must function across a wide range of real-world constraints: changing viewpoints, varying camera heights, rotations, lighting conditions, and occlusions.&lt;/p&gt;
&lt;p&gt;This project posed a challenging question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Can we design a robust, physically deployable adversarial camouflage for vehicles that remains visually consistent with military aesthetics while being consistently misclassified by object detection models?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that although this project focuses on adversarial patches-which overwrite a small region of an image rather than perturbing the entire image-we refer to them broadly as adversarial attacks for simplicity. Patches are a specific subclass of adversarial attacks, optimized for local perturbations with similar misclassification objectives.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Unfortunately, the details of how we constructed and evaluated the physical adversarial camouflages are under NDA, but Figure 1 illustrates the visualization of the goal of the project, which we managed to create an adversarial patch that is both adversarial (misclassified by AI object detector models) and looks military camouflaged. In this project, we not just achieved the highest recorded 3D physical adversarial attack on object detectors, but we also extended the evaluating pipeline significantly to evaluate wider ranges of different objectives, such as different camera resolutions, projections, heights, etc.&lt;/p&gt;
&lt;h2 id=&#34;why-it-matters&#34;&gt;Why It Matters&lt;/h2&gt;
&lt;p&gt;As AI systems like drones, surveillance cameras, and self-driving cars become increasingly common in both military and civilian settings, the ability to physically mislead object detectors through adversarial camouflage raises serious concerns. This work suggests that 3D adversarial textures could be used to evade detection by AI systems in real-world environments—whether to avoid targeting in warfare or to disrupt autonomous vehicles in civilian life. Although specific technical details are under NDA, the implications are clear: physical adversarial attacks represent a credible threat to the safe and reliable deployment of machine learning in high-stakes applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active Learning</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/active-learning/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://philippe-753.github.io/philippebergna.github.io/project/active-learning/</guid>
      <description>&lt;p&gt;COMING SOON!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adversarial Attacks for Facial Verification Systems</title>
      <link>https://philippe-753.github.io/philippebergna.github.io/project/adversarial-attacks-on-facial-verification-systems/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://philippe-753.github.io/philippebergna.github.io/project/adversarial-attacks-on-facial-verification-systems/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;  
  &lt;img src=&#34;featured.png&#34; style=&#34;width:70%; height:auto;&#34; /&gt;  
&lt;p/&gt;  
&lt;p align=&#34;center&#34;&gt;&lt;em&gt;Figure 1: Illustration of how adversarial perturbations can affect face similarity scores. Image adapted from Brown et al., 2020.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of my work at Advai, I contributed to a confidential red-teaming project evaluating the robustness of a commercial facial verification platform—one of the UK’s most widely deployed systems. The focus was on &lt;strong&gt;AI safety&lt;/strong&gt;, specifically understanding how such systems behave under &lt;strong&gt;targeted adversarial attacks&lt;/strong&gt; in a &lt;strong&gt;black-box setting&lt;/strong&gt;, where model internals are unknown.&lt;/p&gt;
&lt;p&gt;The challenge was to simulate how a motivated attacker might manipulate an image to be incorrectly verified as someone else in the system—without triggering biometric defenses like liveness or genuine presence detection. Although technical specifics remain under NDA, the project offered a rare opportunity to test adversarial robustness under realistic deployment conditions.&lt;/p&gt;
&lt;h2 id=&#34;what-are-adversarial-attacks-in-facial-verification&#34;&gt;What Are Adversarial Attacks in Facial Verification?&lt;/h2&gt;
&lt;p&gt;Adversarial attacks on facial verification models aim to subtly modify a face image so that a system misclassifies it—either rejecting a valid user or matching it to someone else. These perturbations are often invisible to humans but exploit vulnerabilities in the model’s learned representations.&lt;/p&gt;
&lt;p&gt;In black-box scenarios, attackers have no access to the model architecture or weights. Instead, they generate attacks on surrogate models and rely on &lt;strong&gt;transferability&lt;/strong&gt;: the hope that perturbations crafted on one model will still fool another.&lt;/p&gt;
&lt;h2 id=&#34;goal&#34;&gt;Goal&lt;/h2&gt;
&lt;p&gt;To generate targeted, transferable adversarial attacks against one of the UK’s top commercial facial verification systems, and evaluate their effectiveness across multiple industry-leading platforms. Simply put, the objective was to test whether an image could be subtly modified to be falsely recognized as a specific person in the system — all without access to their underlying model.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Although full experimental details are under NDA, our strongest targeted attacks successfully induced misclassification under constrained, black-box conditions—without access to model architecture, parameters, or training data. These attacks were generated using open-source surrogates and remained effective when evaluated against commercial systems via standard user-facing APIs.&lt;/p&gt;
&lt;p&gt;The results highlight that even well-established biometric pipelines can be vulnerable to transferable adversarial perturbations in realistic threat scenarios, especially when assurance mechanisms are absent or inconsistently enforced.&lt;/p&gt;
&lt;h2 id=&#34;why-it-matters&#34;&gt;Why It Matters&lt;/h2&gt;
&lt;p&gt;Facial verification systems are increasingly used across borders, banking, and critical infrastructure. This project reinforced that even large-scale commercial systems—deployed by industry leaders—can remain vulnerable to targeted adversarial attacks. These findings underscore the urgent need for proactive AI safety auditing in domains where identity verification underpins national security, privacy, and public trust.&lt;/p&gt;
&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.05790&#34;&gt;Brown et al., 2020&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
